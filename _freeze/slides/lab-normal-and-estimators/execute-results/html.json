{
  "hash": "5ee8df481539e7427e26ae2d7a1b1048",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: | \n  Normal modeling & \\\n  Bayesian estimators\nformat: \n    revealjs:\n      mainfont: Lato\n      smaller: true\n---\n\n\n\n# Normal model\n\n## Exercise\n\n\nLibraries used:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(latex2exp)\n```\n:::\n\n\n\n\n## Data\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbass = read_csv(\"https://sta360-sp25.github.io/data/bass.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(bass)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 171\nColumns: 5\n$ river   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ station <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ length  <dbl> 47.0, 48.7, 55.7, 45.2, 44.7, 43.8, 38.5, 45.8, 44.0, 40.4, 47…\n$ weight  <dbl> 1.616, 1.862, 2.855, 1.199, 1.320, 1.225, 0.870, 1.455, 1.220,…\n$ mercury <dbl> 1.60, 1.50, 1.70, 0.73, 0.56, 0.51, 0.48, 0.95, 1.40, 0.50, 0.…\n```\n\n\n:::\n:::\n\n\n\nMercury, is a naturally occurring element that can have toxic effects on the nervous, digestive and immune systems of humans. Bass from the Waccamaw and Lumber Rivers (NC) were caught randomly, weighed, and measured. In addition, a filet from each fish caught was sent to the lab so that the tissue concentration of mercury could be determined for each fish. Each fish caught corresponds to a single row of the data frame. Today we will examine two columns from the data set: `mercury` (concentration of mercury in ppm) and `weight` (weight of the fish in kg). We'll model the mercury content $y$ of each fish as a function of the fish's weight $x$.\n\n## Model\n\nLet \n\n$$\n\\begin{aligned}\nY_i | \\theta &\\sim \\text{ iid  } N(\\theta x_i, 1)\\\\\n\\theta &\\sim N(\\mu_0, 1 / \\kappa_0)\n\\end{aligned}\n$$\n\nLet $\\mu_0 = 0$, $\\kappa_0 = 1$.\n\n(a). Suppose you observe data $y_1,\\ldots y_n$. Write out the formula for $p(\\theta | y_1, \\ldots y_n)$.\n\n(b). Given the data on the previous slide, use Monte Carlo simulation to plot $p(\\theta | y_1, \\ldots, y_n)$. Additionally, report $E[\\theta | y_1,\\ldots y_n]$ together with a 95% posterior confidence interval.\n\n(c). If you caught a new fish with weight 4kg, what would you predict the mercury content to be? In other words, let x = 4 and compute $E[\\tilde{y}|y_1,\\ldots, y_n, x = 4]$. Additionally, plot the the posterior predictive density $p(\\tilde{y} | y_1, \\ldots y_n, x = 4)$.\n\n(d). Critique your model. Hint: compare to the models below:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(mercury ~ weight, data = bass)\nlm(mercury ~ 0 + weight, data = bass)\n```\n:::\n\n\n\n\n## Solution (a)\n\n#### a\n\n$$\n\\theta |  y_1, \\ldots y_n \\sim N(\\mu_n, \\tau_n^2)\n$$\n<!-- \\theta | \\sigma^2, y_1, \\ldots y_n \\sim N(\\mu_n, \\tau_n^2) -->\n\n\nwhere \n\n<!-- \\begin{aligned} -->\n<!-- \\mu_n &= \\frac{\\kappa_0 \\mu_0 + \\sum y_i x_i}{\\kappa_0 + \\sum x_i^2}\\\\ -->\n<!-- \\tau_n^2 &= \\frac{\\sigma^2}{\\kappa_0 + \\sum x_i^2} -->\n<!-- \\end{aligned} -->\n\n$$\n\\begin{aligned}\n\\mu_n &= \\frac{\\kappa_0 \\mu_0 + \\sum y_i x_i}{\\kappa_0 + \\sum x_i^2}\\\\\n\\tau_n^2 &= \\frac{1}{\\kappa_0 + \\sum x_i^2}\n\\end{aligned}\n$$\n\n<!-- ## Solution (b) -->\n\n<!-- #### b -->\n\n<!-- $$ -->\n<!-- \\nu_n = \\nu_0 + n -->\n<!-- $$ -->\n<!-- $$ -->\n<!-- \\sigma_n^2 = \\frac{1}{\\nu_n} \\left[\\sum y_i^2 - \\kappa_0 \\mu_0^2 + \\frac{\\left(\\sum y_i x_i + \\kappa_0 \\mu_0\\right)^2}{\\left( \\sum x_i^2 + \\kappa_0  \\right)} -->\n<!-- \\right] -->\n<!-- $$ -->\n\n## Solution (b) \n\n#### b\n\n::: panel-tabset\n\n## demo\n\nDemo with simulated data to make sure code works: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulated data\nset.seed(123)\ntrue.theta = 4\ntrue.sigma = 1\nN = 10\nx = seq(from = 1, to = 10, length = N)\ny = rnorm(N, true.theta * x, true.sigma)\n\n# prior parameters\nk0 = 1\nmu0 = 0\n\nsumYX = sum(y * x)\nd = (k0 + sum(x^2))\nmun = ((k0 * mu0) + sumYX) / d\ntn = sqrt(1 / d)\n\ntheta.postsample = rnorm(10000, mun, tn)\nhist(theta.postsample)\n```\n:::\n\n\n\n## demo plot \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lab-normal-and-estimators_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n\n## solution code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = bass$weight\ny = bass$mercury\n\n# prior parameters\nk0 = 1\nmu0 = 0\n\nsumYX = sum(y * x)\nd = (k0 + sum(x^2))\nmun = ((k0 * mu0) + sumYX) / d\ntn = sqrt(1 / d)\n\ntheta.postsample = rnorm(10000, mun, tn)\nhist(theta.postsample)\n```\n:::\n\n\n\n\n## solution plot\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lab-normal-and-estimators_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n## summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(theta.postsample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8314533\n```\n\n\n:::\n\n```{.r .cell-code}\nquantile(theta.postsample, c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     2.5%     97.5% \n0.7284780 0.9356104 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n## Solution (c)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use posterior samples of theta and x = 4 to simulate ytilde\n\nytilde = rnorm(10000, theta.postsample * 4, 1)\nhist(ytilde)\n```\n\n::: {.cell-output-display}\n![](lab-normal-and-estimators_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n\n```{.r .cell-code}\nmean(ytilde)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.319342\n```\n\n\n:::\n:::\n\n\n\nThis matches intuition (law of total expectation gives the closed form solution: 4 * 0.838 = 3.352).\n\n## Solution (d)\n\nWe have no intercept term. We are assuming that our regression line goes through the origin. This is a strong assumption. Our model will be most similar to the `lm` model without an intercept term: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(mercury ~ 0 + weight, data = bass)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mercury ~ 0 + weight, data = bass)\n\nCoefficients:\nweight  \n0.8343  \n```\n\n\n:::\n:::\n\n\n\nHowever, we'll get a different estimate of $\\hat{\\theta}$ if we include an intercept term,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(mercury ~ weight, data = bass)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mercury ~ weight, data = bass)\n\nCoefficients:\n(Intercept)       weight  \n     0.6387       0.4818  \n```\n\n\n:::\n:::\n\n\n\n# Estimators\n\n\n## Exercises\n\n## Exercise 1: estimators\n\nLet $Y_1,\\ldots Y_n$ be iid random variables with expectation $\\theta$ and variance $\\sigma^2$.\n\nShow that $\\frac{1}{n} \\sum_{i = 1}^n (Y_i -\\bar{Y})^2$ is a biased estimator of $\\sigma^2$.\n\n\n## Exercise 2: estimators\n\n$$\n\\begin{aligned}\nY_1, \\ldots, Y_n &\\sim \\text{ i.i.d. binary}(\\theta)\\\\\n\\theta &\\sim \\text{beta}(a, b)\n\\end{aligned}\n$$\n\n\n\n- Compute $\\hat{\\theta}_{MLE}$\n- Compute $\\hat{\\theta}_{B} = E[\\theta | y_1,\\ldots y_n]$.\n- Compare $MSE(\\hat{\\theta}_{MLE})$ to $MSE(\\hat{\\theta}_{B})$). Under what conditions is the MSE of $\\hat{\\theta}_B$ smaller?\n\n<!-- ## Exercise 3: MVN -->\n\n<!-- Consider a single observation $(y_1, y_2)$ drawn from a bivariate normal distribution with mean $(\\theta_1, \\theta_2)$ and fixed, known $2 \\times 2$ covariance matrix $\\Sigma = \\left[ {\\begin{array}{cc} -->\n<!--    1 & .5 \\\\ -->\n<!--    .5 & 1 -->\n<!--   \\end{array} } \\right]$. Consider a uniform prior on $\\theta = (\\theta_1, \\theta_2)$ : $p(\\theta_1, \\theta_2) \\propto 1$.  -->\n\n<!-- (a.) Derive the joint posterior for $\\theta_1, \\theta_2 | y_1, y_2, \\Sigma$. Describe a direct sampler for this distribution. -->\n\n<!-- (b.) Write down full conditionals $p(\\theta_1 | \\theta_2, y_1, y_2, \\Sigma)$ and $p(\\theta_2 | \\theta_1, y_1, y_2, \\Sigma)$. Write pseudo-code to describe a Gibbs sampling procedure. Hint: you can use the result from HW6 Ex 3. -->\n\n<!-- (c.) Will the direct sampler from part (a) or the Gibbs sampler in part (b) have higher ESS? Why? -->\n\n# Solutions\n\n## Solution 1\n\nLet $\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i = 1}^n (Y_i -\\bar{Y})^2$.\n\n$$\n\\begin{aligned}\nBias(\\hat{\\sigma}^2 | \\sigma^2 = \\sigma_0^2) &= E[\\hat{\\sigma}^2|\\sigma_0^2] - \\sigma_0^2\\\\\n&=  - \\sigma_0^2 + \\frac{1}{n} \\sum_{i = 1}^n E[(Y_i -\\bar{Y})^2|\\sigma_0^2]\\\\\n&= - \\sigma_0^2 + \\frac{1}{n} \\sum_{i=1}^n \\left[\nE[Y_i^2 |\\sigma_0^2] - 2E[Y_i \\bar{Y}|\\sigma_0^2] + E[\\bar{Y}^2 | \\sigma_0^2]\n\\right]\n\\end{aligned}\n$$\n\nRecall that for any random variable X, $Var(X) = E[X^2] - E[X]^2$. Using this fact, we continue our proof above:\n\n$$\n\\begin{aligned}\n&= -\\sigma_0^2 +(\\sigma_0^2  + \\theta^2) \n-2 \\frac{1}{n} \\sum_{i=1}^n \\left[  E~\\left(Y_i \\frac{1}{n}\\sum_j Y_j\\right) | \\sigma_0^2    \\right]\n+ \\left(\\frac{\\sigma_0^2}{n} + \\theta^2\\right)\\\\\n&= 2\\theta^2 + \\frac{\\sigma_0^2}{n} - \\frac{2}{n} \n\\left(n\\theta^2  - \\sigma_0^2\n\\right)\\\\\n&= \\frac{(n-1)\\sigma_0^2}{n}\n\\end{aligned}\n$$\n\n## Solution 2\n\n$$\n\\begin{aligned}\n\\hat{\\theta}_{MLE} &= \\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\\\\n\\hat{\\theta}_B &= \\frac{n\\bar{y}+a}{n+a+b} = \n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\nMSE(\\hat{\\theta}_{MLE}|\\theta) &= \\frac{\\theta(1-\\theta)}{n}\\\\\nMSE(\\hat{\\theta}_B|\\theta) &= \\frac{n}{n + a + b}\\bar{Y} + \\frac{a + b}{n + a + b} \\frac{a}{a + b} =  w \\bar{Y} + (1-w)\\frac{a}{a+b}\\\\\n&= w^2Var(\\bar{Y} | \\theta) +  (1-w)^2 \\left(\\frac{a}{a+b} - \\theta\\right)^2\\\\\n&= {w^2} \\frac{\\theta(1-\\theta)}{n} + (1-w)^2  \\left(\\frac{a}{a+b} - \\theta\\right)^2\n\\end{aligned}\n$$\nFor the Bayesian estimator to have smaller MSE than the MLE, we need\n\n$$\n\\begin{aligned}\n \\left(\\frac{a}{a+b} - \\theta\\right)^2 &\\leq \\frac{\\theta(1 - \\theta)}{n} \\frac{1 + w}{1 - w}\\\\\n &\\leq \\frac{\\theta(1 - \\theta) (2n + a + b)}{n(a+b)}\n \\end{aligned}\n$$\n\nIn words, if our prior guess $a / (a+b)$ is \"close enough\" to $\\theta$, where \"close enough\" is defined by the inequality above and is proportional to the variance of the estimator, then the MSE of the Bayesian estimator is smaller. \n\n",
    "supporting": [
      "lab-normal-and-estimators_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}